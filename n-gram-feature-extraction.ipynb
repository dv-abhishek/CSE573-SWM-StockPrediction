{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download(\"popular\")\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized news data\n",
    "\n",
    "with open('AaplLabelledNewsData-OrderPreserved.pkl', 'rb') as f:\n",
    "    labelled_aapl_news_df = pickle.load(f)\n",
    "    \n",
    "with open('AmznLabelledNewsData-OrderPreserved.pkl', 'rb') as f:\n",
    "    labelled_amzn_news_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_timestamp</th>\n",
       "      <th>stock_timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-07 20:00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>businesswire.com</td>\n",
       "      <td>[nape, summit, also, annual, luncheon, tom, sp...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-08 21:37:00</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[huge, x, comeback, tour, follow, note, big, y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-12 01:55:00</td>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[browser, os, ad, server, make, first, move, c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-12 22:10:00</td>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>investingnews.com</td>\n",
       "      <td>[humanitarian, place, conflict, free, region]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-14 11:42:00</td>\n",
       "      <td>2017-12-15</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>[prefer, invest]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       news_timestamp stock_timestamp             source  \\\n",
       "0 2017-12-07 20:00:00      2017-12-08   businesswire.com   \n",
       "1 2017-12-08 21:37:00      2017-12-11          yahoo.com   \n",
       "2 2017-12-12 01:55:00      2017-12-13          yahoo.com   \n",
       "3 2017-12-12 22:10:00      2017-12-13  investingnews.com   \n",
       "4 2017-12-14 11:42:00      2017-12-15   seekingalpha.com   \n",
       "\n",
       "                                              tokens label  \n",
       "0  [nape, summit, also, annual, luncheon, tom, sp...    -1  \n",
       "1  [huge, x, comeback, tour, follow, note, big, y...     1  \n",
       "2  [browser, os, ad, server, make, first, move, c...     1  \n",
       "3      [humanitarian, place, conflict, free, region]     1  \n",
       "4                                   [prefer, invest]     1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize data\n",
    "labelled_aapl_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_timestamp</th>\n",
       "      <th>stock_timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-15 12:04:00</td>\n",
       "      <td>2017-12-18</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>[amazon, soon, sell, pull, e, platform, two, y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-21 12:04:00</td>\n",
       "      <td>2017-12-22</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>[line, cashier, similar, fashion, futurist, am...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-21 12:27:00</td>\n",
       "      <td>2017-12-22</td>\n",
       "      <td>thestreet.com</td>\n",
       "      <td>[fit, fang, stock, amazon, alphabet, drug, int...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-22 00:00:00</td>\n",
       "      <td>2017-12-26</td>\n",
       "      <td>investors.com</td>\n",
       "      <td>[amazon, cloud, china]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-22 20:57:00</td>\n",
       "      <td>2017-12-26</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[stream, stick, plus, offer, play, amazon, con...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       news_timestamp stock_timestamp            source  \\\n",
       "0 2017-12-15 12:04:00      2017-12-18  seekingalpha.com   \n",
       "1 2017-12-21 12:04:00      2017-12-22  seekingalpha.com   \n",
       "2 2017-12-21 12:27:00      2017-12-22     thestreet.com   \n",
       "3 2017-12-22 00:00:00      2017-12-26     investors.com   \n",
       "4 2017-12-22 20:57:00      2017-12-26         yahoo.com   \n",
       "\n",
       "                                              tokens label  \n",
       "0  [amazon, soon, sell, pull, e, platform, two, y...     1  \n",
       "1  [line, cashier, similar, fashion, futurist, am...    -1  \n",
       "2  [fit, fang, stock, amazon, alphabet, drug, int...    -1  \n",
       "3                             [amazon, cloud, china]     1  \n",
       "4  [stream, stick, plus, offer, play, amazon, con...     1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_amzn_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 rows for apple\n",
      "Completed 10000 rows for apple\n",
      "Completed 20000 rows for apple\n",
      "Completed 30000 rows for apple\n",
      "Completed 40000 rows for apple\n",
      "Completed 50000 rows for apple\n",
      "Completed 60000 rows for apple\n",
      "Completed 70000 rows for apple\n",
      "Completed 0 rows for amazon\n",
      "Completed 5000 rows for amazon\n",
      "Completed 10000 rows for amazon\n",
      "Completed 15000 rows for amazon\n"
     ]
    }
   ],
   "source": [
    "columns = ['merged_tokens']\n",
    "corpus_aapl = []\n",
    "corpus_amzn = []\n",
    "\n",
    "#can try to vectorize this\n",
    "for index, row in labelled_aapl_news_df.iterrows():\n",
    "    corpus_aapl.append(' '.join(row['tokens']))\n",
    "    if index % 10000 == 0:\n",
    "        print(\"Completed %d rows for apple\" % index)\n",
    "    \n",
    "for index, row in labelled_amzn_news_df.iterrows():\n",
    "    corpus_amzn.append(' '.join(row['tokens']))\n",
    "    if index % 5000 == 0:\n",
    "        print(\"Completed %d rows for amazon\" % index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nape summit also annual luncheon tom speaker new job fair upstream profession seminar present organ seg intern theater run two day prospect preview near exhibit space',\n",
       " 'huge x comeback tour follow note big year best phone money one buy premium start cool per month depend plan put reach mani spot face id facial fantast dual len camera help make kind need time analyst question sure loss home button disappear could full screen handset turn user slick touch control even tire go high power second plus lover impress display got great two mean much fan',\n",
       " 'browser os ad server make first move competitor like follow lead context ecosystem rife fraud publish ought much kind low rent unit reader rob hacker might target plan kill net neutral affect still use x face id even though say beat big media serf land tech giant sold buy twitter',\n",
       " 'humanitarian place conflict free region',\n",
       " 'prefer invest']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print sample items to ensure update\n",
    "corpus_aapl[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazon soon sell pull e platform two year ago support prime video',\n",
       " 'line cashier similar fashion futurist amazon go store year ago yet open public',\n",
       " 'fit fang stock amazon alphabet drug intern',\n",
       " 'amazon cloud china',\n",
       " 'stream stick plus offer play amazon content get access prime video fire interact expect exist go latest iter cut diamond shape port also come assist built current usual price product look like soon away said pull jan stop playground control smart home final bring box might hope ultra spec show thank back forth cast bummer']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_amzn[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['access', 'accord', 'account', 'across', 'action', 'actual', 'ad', 'add', 'address', 'adjust', 'advisor', 'affect', 'afford', 'afternoon', 'aggress', 'ago', 'ahead', 'air', 'alert', 'allow', 'almost', 'along', 'alphabet', 'also', 'although', 'alway', 'amazon', 'america', 'american', 'amid']\n",
      "Aapl one-gram feature vector size: 71941 X 703\n"
     ]
    }
   ],
   "source": [
    "aapl_one_gram_vectorizer = CountVectorizer(max_df=0.9, min_df=0.01, stop_words=stop_words, ngram_range=(1,1))\n",
    "aapl_one_gram_features = aapl_one_gram_vectorizer.fit_transform(corpus_aapl)\n",
    "\n",
    "#printing first 30 feature words for visualization\n",
    "print(aapl_one_gram_vectorizer.get_feature_names()[:30])\n",
    "\n",
    "labelled_aapl_news_df = labelled_aapl_news_df.rename(columns={'tokens': 'features'})\n",
    "\n",
    "aapl_one_gram_features_array = aapl_one_gram_features.toarray()\n",
    "print(\"Aapl one-gram feature vector size: %d X %d\" % (len(aapl_one_gram_features_array), len(aapl_one_gram_features_array[0])))\n",
    "\n",
    "for i in range(len(aapl_one_gram_features_array)):\n",
    "    labelled_aapl_news_df.at[i,'features'] = aapl_one_gram_features_array[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_timestamp</th>\n",
       "      <th>stock_timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-07 20:00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>businesswire.com</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-08 21:37:00</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-12 01:55:00</td>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-12 22:10:00</td>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>investingnews.com</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-14 11:42:00</td>\n",
       "      <td>2017-12-15</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       news_timestamp stock_timestamp             source  \\\n",
       "0 2017-12-07 20:00:00      2017-12-08   businesswire.com   \n",
       "1 2017-12-08 21:37:00      2017-12-11          yahoo.com   \n",
       "2 2017-12-12 01:55:00      2017-12-13          yahoo.com   \n",
       "3 2017-12-12 22:10:00      2017-12-13  investingnews.com   \n",
       "4 2017-12-14 11:42:00      2017-12-15   seekingalpha.com   \n",
       "\n",
       "                                            features label  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    -1  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n",
       "2  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, ...     1  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print sample frames\n",
    "labelled_aapl_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['access', 'accord', 'account', 'acronym', 'across', 'act', 'action', 'actual', 'ad', 'add', 'adjust', 'advisor', 'ago', 'ahead', 'ai', 'aim', 'alert', 'allow', 'almost', 'along', 'alphabet', 'also', 'although', 'america', 'american', 'amid', 'among', 'analyst', 'annual', 'appear']\n",
      "Amzn one-gram feature vector size: 19916 X 576\n"
     ]
    }
   ],
   "source": [
    "amzn_one_gram_vectorizer = CountVectorizer(max_df=0.9, min_df=0.01, stop_words=stop_words, ngram_range=(1,1))\n",
    "amzn_one_gram_features = amzn_one_gram_vectorizer.fit_transform(corpus_amzn)\n",
    "\n",
    "#printing first 30 feature words for visualization\n",
    "print(amzn_one_gram_vectorizer.get_feature_names()[:30])\n",
    "\n",
    "labelled_amzn_news_df = labelled_amzn_news_df.rename(columns={'tokens': 'features'})\n",
    "\n",
    "amzn_one_gram_features_array = amzn_one_gram_features.toarray()\n",
    "print(\"Amzn one-gram feature vector size: %d X %d\" % (len(amzn_one_gram_features_array), len(amzn_one_gram_features_array[0])))\n",
    "\n",
    "for i in range(len(amzn_one_gram_features_array)):\n",
    "    labelled_amzn_news_df.at[i,'features'] = amzn_one_gram_features_array[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_timestamp</th>\n",
       "      <th>stock_timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-15 12:04:00</td>\n",
       "      <td>2017-12-18</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-21 12:04:00</td>\n",
       "      <td>2017-12-22</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-21 12:27:00</td>\n",
       "      <td>2017-12-22</td>\n",
       "      <td>thestreet.com</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-22 00:00:00</td>\n",
       "      <td>2017-12-26</td>\n",
       "      <td>investors.com</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-22 20:57:00</td>\n",
       "      <td>2017-12-26</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       news_timestamp stock_timestamp            source  \\\n",
       "0 2017-12-15 12:04:00      2017-12-18  seekingalpha.com   \n",
       "1 2017-12-21 12:04:00      2017-12-22  seekingalpha.com   \n",
       "2 2017-12-21 12:27:00      2017-12-22     thestreet.com   \n",
       "3 2017-12-22 00:00:00      2017-12-26     investors.com   \n",
       "4 2017-12-22 20:57:00      2017-12-26         yahoo.com   \n",
       "\n",
       "                                            features label  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...     1  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...    -1  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    -1  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print sample frames\n",
    "labelled_amzn_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save one-gram features labelled data\n",
    "\n",
    "with open('aapl_one_gram_features_labelled_df.pkl', 'wb') as f:\n",
    "    pickle.dump(labelled_aapl_news_df, f)\n",
    "with open('amzn_one_gram_features_labelled_df.pkl', 'wb') as f:\n",
    "    pickle.dump(labelled_amzn_news_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa aal', 'aal ab', 'ab abb', 'abb acor', 'absurd cheap', 'access design', 'access digit', 'accord analyst', 'accord data', 'accord earn', 'accord file', 'accord firm', 'accord given', 'accord maintain', 'accord market', 'accord new', 'accord recent', 'accord report', 'accord research', 'accord stock', 'accord street', 'account boost', 'account grew', 'account hold', 'account invest', 'account lift', 'account make', 'account near', 'account portfolio', 'account rais']\n",
      "Aapl two-gram feature vector size: 71941 X 3831\n"
     ]
    }
   ],
   "source": [
    "#extracting two-gram features now\n",
    "\n",
    "aapl_two_gram_vectorizer = CountVectorizer(max_df=0.9, min_df=0.001, stop_words=stop_words, ngram_range=(2,2))\n",
    "aapl_two_gram_features = aapl_two_gram_vectorizer.fit_transform(corpus_aapl)\n",
    "\n",
    "#printing first 30 feature words for visualization\n",
    "print(aapl_two_gram_vectorizer.get_feature_names()[:30])\n",
    "\n",
    "aapl_two_gram_features_array = aapl_two_gram_features.toarray()\n",
    "print(\"Aapl two-gram feature vector size: %d X %d\" % (len(aapl_two_gram_features_array), len(aapl_two_gram_features_array[0])))\n",
    "\n",
    "for i in range(len(aapl_two_gram_features_array)):\n",
    "    labelled_aapl_news_df.at[i,'features'] = aapl_two_gram_features_array[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save two-gram features labelled data for Apple\n",
    "\n",
    "with open('aapl_two_gram_features_labelled_df.pkl', 'wb') as f:\n",
    "    pickle.dump(labelled_aapl_news_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa aal', 'aal ab', 'ab abb', 'abb acor', 'access person', 'access premium', 'accord data', 'accord file', 'accord maintain', 'accord new', 'accord recent', 'accord report', 'accord research', 'accord street', 'account near', 'account portal', 'account total', 'acor act', 'acronym amazon', 'acronym popular', 'acronym quintet', 'acronym refer', 'across retail', 'act ad', 'action alert', 'action might', 'action plan', 'actual previous', 'ad ak', 'ad amazon']\n",
      "Amzn two-gram feature vector size: 19916 X 2685\n"
     ]
    }
   ],
   "source": [
    "amzn_two_gram_vectorizer = CountVectorizer(max_df=0.9, min_df=0.001, stop_words=stop_words, ngram_range=(2,2))\n",
    "amzn_two_gram_features = amzn_two_gram_vectorizer.fit_transform(corpus_amzn)\n",
    "\n",
    "#printing first 30 feature words for visualization\n",
    "print(amzn_two_gram_vectorizer.get_feature_names()[:30])\n",
    "\n",
    "amzn_two_gram_features_array = amzn_two_gram_features.toarray()\n",
    "print(\"Amzn two-gram feature vector size: %d X %d\" % (len(amzn_two_gram_features_array), len(amzn_two_gram_features_array[0])))\n",
    "\n",
    "for i in range(len(amzn_two_gram_features_array)):\n",
    "    labelled_amzn_news_df.at[i,'features'] = amzn_two_gram_features_array[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save two-gram features labelled data for Amazon\n",
    "\n",
    "with open('amzn_two_gram_features_labelled_df.pkl', 'wb') as f:\n",
    "    pickle.dump(labelled_amzn_news_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
