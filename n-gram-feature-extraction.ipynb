{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized news data\n",
    "\n",
    "with open('AaplLabelledNewsData-OrderPreserved.pkl', 'rb') as f:\n",
    "    labelled_aapl_news_df = pickle.load(f)\n",
    "    \n",
    "with open('AmznLabelledNewsData-OrderPreserved.pkl', 'rb') as f:\n",
    "    labelled_amzn_news_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       news_timestamp stock_timestamp             source  \\\n",
      "0 2017-12-07 20:00:00      2017-12-08   businesswire.com   \n",
      "1 2017-12-08 21:37:00      2017-12-11          yahoo.com   \n",
      "2 2017-12-12 01:55:00      2017-12-13          yahoo.com   \n",
      "3 2017-12-12 22:10:00      2017-12-13  investingnews.com   \n",
      "4 2017-12-14 11:42:00      2017-12-15   seekingalpha.com   \n",
      "5 2017-12-14 12:31:00      2017-12-15   seekingalpha.com   \n",
      "6 2017-12-15 02:32:00      2017-12-18   businesswire.com   \n",
      "7 2017-12-15 12:04:00      2017-12-18   seekingalpha.com   \n",
      "8 2017-12-18 20:05:00      2017-12-19   seekingalpha.com   \n",
      "9 2017-12-21 00:00:00      2017-12-22    morningstar.com   \n",
      "\n",
      "                                              tokens label  \n",
      "0  [nape, summit, also, annual, luncheon, tom, sp...    -1  \n",
      "1  [huge, x, comeback, tour, follow, note, big, y...     1  \n",
      "2  [browser, os, ad, server, make, first, move, c...     1  \n",
      "3      [humanitarian, place, conflict, free, region]     1  \n",
      "4                                   [prefer, invest]     1  \n",
      "5  [invest, good, news, fund, new, job, refurbish...     1  \n",
      "6  [park, made, new, invest, four, transact, cisc...     1  \n",
      "7  [amazon, soon, sell, pull, e, platform, two, y...     1  \n",
      "8  [said, pandora, p, sirius, cumulus, player, ra...    -1  \n",
      "9  [read, earn, north, america, may, start, gain,...     1  \n",
      "       news_timestamp stock_timestamp            source  \\\n",
      "0 2017-12-15 12:04:00      2017-12-18  seekingalpha.com   \n",
      "1 2017-12-21 12:04:00      2017-12-22  seekingalpha.com   \n",
      "2 2017-12-21 12:27:00      2017-12-22     thestreet.com   \n",
      "3 2017-12-22 00:00:00      2017-12-26     investors.com   \n",
      "4 2017-12-22 20:57:00      2017-12-26         yahoo.com   \n",
      "5 2017-12-27 00:00:00      2017-12-28     investors.com   \n",
      "6 2017-12-27 11:43:00      2017-12-28  seekingalpha.com   \n",
      "7 2017-12-28 00:00:00      2017-12-29     investors.com   \n",
      "8 2017-12-28 11:39:00      2017-12-29  seekingalpha.com   \n",
      "9 2017-12-28 11:39:00      2017-12-29  seekingalpha.com   \n",
      "\n",
      "                                              tokens label  \n",
      "0  [amazon, soon, sell, pull, e, platform, two, y...     1  \n",
      "1  [line, cashier, similar, fashion, futurist, am...    -1  \n",
      "2  [fit, fang, stock, amazon, alphabet, drug, int...    -1  \n",
      "3                             [amazon, cloud, china]     1  \n",
      "4  [stream, stick, plus, offer, play, amazon, con...     1  \n",
      "5  [new, year, find, almost, shape, read, stock, ...    -1  \n",
      "6  [amazon, ad, product, platform, push, could, t...    -1  \n",
      "7  [amazon, strong, holiday, result, rose, report...    -1  \n",
      "8  [crown, bin, look, give, kingdom, high, tech, ...    -1  \n",
      "9  [crown, bin, look, give, kingdom, high, tech, ...    -1  \n"
     ]
    }
   ],
   "source": [
    "#print sample frames\n",
    "print(labelled_aapl_news_df.head(10))\n",
    "print(labelled_amzn_news_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 rows for apple\n",
      "Completed 10000 rows for apple\n",
      "Completed 20000 rows for apple\n",
      "Completed 30000 rows for apple\n",
      "Completed 40000 rows for apple\n",
      "Completed 50000 rows for apple\n",
      "Completed 60000 rows for apple\n",
      "Completed 70000 rows for apple\n",
      "Completed 0 rows for amazon\n",
      "Completed 5000 rows for amazon\n",
      "Completed 10000 rows for amazon\n",
      "Completed 15000 rows for amazon\n"
     ]
    }
   ],
   "source": [
    "columns = ['merged_tokens']\n",
    "corpus_aapl = []\n",
    "corpus_amzn = []\n",
    "\n",
    "#can try to vectorize this\n",
    "for index, row in labelled_aapl_news_df.iterrows():\n",
    "    corpus_aapl.append(' '.join(row['tokens']))\n",
    "    if index % 10000 == 0:\n",
    "        print(\"Completed %d rows for apple\" % index)\n",
    "    \n",
    "for index, row in labelled_amzn_news_df.iterrows():\n",
    "    corpus_amzn.append(' '.join(row['tokens']))\n",
    "    if index % 5000 == 0:\n",
    "        print(\"Completed %d rows for amazon\" % index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nape summit also annual luncheon tom speaker new job fair upstream profession seminar present organ seg intern theater run two day prospect preview near exhibit space',\n",
       " 'huge x comeback tour follow note big year best phone money one buy premium start cool per month depend plan put reach mani spot face id facial fantast dual len camera help make kind need time analyst question sure loss home button disappear could full screen handset turn user slick touch control even tire go high power second plus lover impress display got great two mean much fan',\n",
       " 'browser os ad server make first move competitor like follow lead context ecosystem rife fraud publish ought much kind low rent unit reader rob hacker might target plan kill net neutral affect still use x face id even though say beat big media serf land tech giant sold buy twitter',\n",
       " 'humanitarian place conflict free region',\n",
       " 'prefer invest']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print sample items to ensure update\n",
    "corpus_aapl[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazon soon sell pull e platform two year ago support prime video',\n",
       " 'line cashier similar fashion futurist amazon go store year ago yet open public',\n",
       " 'fit fang stock amazon alphabet drug intern',\n",
       " 'amazon cloud china',\n",
       " 'stream stick plus offer play amazon content get access prime video fire interact expect exist go latest iter cut diamond shape port also come assist built current usual price product look like soon away said pull jan stop playground control smart home final bring box might hope ultra spec show thank back forth cast bummer']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_amzn[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aal', 'aam', 'aaron', 'ab', 'aba', 'aback', 'abacus', 'abandon', 'abattoir', 'abb', 'abbasi', 'abbey', 'abbot', 'abe', 'abel', 'aberdeen', 'abhor', 'abidi', 'abitibi', 'abner', 'aboard', 'abolish', 'abord', 'abort', 'abound', 'abraham', 'abram', 'abreast', 'abroad']\n",
      "Aapl one-gram feature vector size: 71941 X 9486\n"
     ]
    }
   ],
   "source": [
    "aapl_one_gram_vectorizer = CountVectorizer()\n",
    "aapl_one_gram_features = aapl_one_gram_vectorizer.fit_transform(corpus_aapl)\n",
    "\n",
    "#printing first 30 feature words for visualization\n",
    "print(aapl_one_gram_vectorizer.get_feature_names()[:30])\n",
    "\n",
    "labelled_aapl_news_df = labelled_aapl_news_df.rename(columns={'tokens': 'features'})\n",
    "\n",
    "aapl_one_gram_features_array = aapl_one_gram_features.toarray()\n",
    "print(\"Aapl one-gram feature vector size: %d X %d\" % (len(aapl_one_gram_features_array), len(aapl_one_gram_features_array[0])))\n",
    "\n",
    "for i in range(len(aapl_one_gram_features_array)):\n",
    "    labelled_aapl_news_df.at[i,'features'] = aapl_one_gram_features_array[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       news_timestamp stock_timestamp             source  \\\n",
      "0 2017-12-07 20:00:00      2017-12-08   businesswire.com   \n",
      "1 2017-12-08 21:37:00      2017-12-11          yahoo.com   \n",
      "2 2017-12-12 01:55:00      2017-12-13          yahoo.com   \n",
      "3 2017-12-12 22:10:00      2017-12-13  investingnews.com   \n",
      "4 2017-12-14 11:42:00      2017-12-15   seekingalpha.com   \n",
      "5 2017-12-14 12:31:00      2017-12-15   seekingalpha.com   \n",
      "6 2017-12-15 02:32:00      2017-12-18   businesswire.com   \n",
      "7 2017-12-15 12:04:00      2017-12-18   seekingalpha.com   \n",
      "8 2017-12-18 20:05:00      2017-12-19   seekingalpha.com   \n",
      "9 2017-12-21 00:00:00      2017-12-22    morningstar.com   \n",
      "\n",
      "                                            features label  \n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    -1  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n",
      "5  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n",
      "6  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n",
      "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n",
      "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    -1  \n",
      "9  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n"
     ]
    }
   ],
   "source": [
    "#print sample frames\n",
    "print(labelled_aapl_news_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aal', 'aaron', 'ab', 'abacus', 'abandon', 'abb', 'abbasi', 'abbey', 'abduct', 'abel', 'aberdeen', 'abner', 'aboard', 'abord', 'abound', 'abraham', 'abram', 'abroad', 'abrupt', 'absent', 'absorb', 'abstract', 'absurd', 'abu', 'abuzz', 'abyss', 'acacia', 'acapulco', 'accent']\n",
      "Amzn one-gram feature vector size: 19916 X 6420\n"
     ]
    }
   ],
   "source": [
    "amzn_one_gram_vectorizer = CountVectorizer()\n",
    "amzn_one_gram_features = amzn_one_gram_vectorizer.fit_transform(corpus_amzn)\n",
    "\n",
    "#printing first 30 feature words for visualization\n",
    "print(amzn_one_gram_vectorizer.get_feature_names()[:30])\n",
    "\n",
    "labelled_amzn_news_df = labelled_amzn_news_df.rename(columns={'tokens': 'features'})\n",
    "\n",
    "amzn_one_gram_features_array = amzn_one_gram_features.toarray()\n",
    "print(\"Amzn one-gram feature vector size: %d X %d\" % (len(amzn_one_gram_features_array), len(amzn_one_gram_features_array[0])))\n",
    "\n",
    "for i in range(len(amzn_one_gram_features_array)):\n",
    "    labelled_amzn_news_df.at[i,'features'] = amzn_one_gram_features_array[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       news_timestamp stock_timestamp            source  \\\n",
      "0 2017-12-15 12:04:00      2017-12-18  seekingalpha.com   \n",
      "1 2017-12-21 12:04:00      2017-12-22  seekingalpha.com   \n",
      "2 2017-12-21 12:27:00      2017-12-22     thestreet.com   \n",
      "3 2017-12-22 00:00:00      2017-12-26     investors.com   \n",
      "4 2017-12-22 20:57:00      2017-12-26         yahoo.com   \n",
      "5 2017-12-27 00:00:00      2017-12-28     investors.com   \n",
      "6 2017-12-27 11:43:00      2017-12-28  seekingalpha.com   \n",
      "7 2017-12-28 00:00:00      2017-12-29     investors.com   \n",
      "8 2017-12-28 11:39:00      2017-12-29  seekingalpha.com   \n",
      "9 2017-12-28 11:39:00      2017-12-29  seekingalpha.com   \n",
      "\n",
      "                                            features label  \n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    -1  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    -1  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n",
      "5  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    -1  \n",
      "6  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    -1  \n",
      "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    -1  \n",
      "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    -1  \n",
      "9  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    -1  \n"
     ]
    }
   ],
   "source": [
    "#print sample frames\n",
    "print(labelled_amzn_news_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save one-gram features labelled data\n",
    "\n",
    "with open('aapl_one_gram_features_labelled_df.pkl', 'wb') as f:\n",
    "    pickle.dump(labelled_aapl_news_df, f)\n",
    "with open('amzn_one_gram_features_labelled_df.pkl', 'wb') as f:\n",
    "    pickle.dump(labelled_amzn_news_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa aal', 'aa ade', 'aa auto', 'aa automat', 'aa azo', 'aa bac', 'aa biggest', 'aa ce', 'aa champion', 'aa col', 'aa credit', 'aa flight', 'aa flo', 'aa global', 'aa group', 'aa hum', 'aa josh', 'aa like', 'aa load', 'aa loop', 'aa meet', 'aa morgan', 'aa na', 'aa narrow', 'aa nation', 'aa need', 'aa non', 'aa one', 'aa partnership', 'aa percent']\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 253. GiB for an array with shape (71941, 472850) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-d87c90a793d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maapl_two_gram_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Aapl two-gram feature vector size: %d X %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maapl_two_gram_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maapl_two_gram_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maapl_two_gram_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 253. GiB for an array with shape (71941, 472850) and data type int64"
     ]
    }
   ],
   "source": [
    "#extracting two-gram features now\n",
    "\n",
    "aapl_two_gram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "aapl_two_gram_features = aapl_two_gram_vectorizer.fit_transform(corpus_aapl)\n",
    "\n",
    "#printing first 30 feature words for visualization\n",
    "print(aapl_two_gram_vectorizer.get_feature_names()[:30])\n",
    "\n",
    "aapl_two_gram_features_array = aapl_two_gram_features.toarray()\n",
    "print(\"Aapl two-gram feature vector size: %d X %d\" % (len(aapl_two_gram_features_array), len(aapl_two_gram_features_array[0])))\n",
    "\n",
    "for i in range(len(aapl_two_gram_features_array)):\n",
    "    labelled_aapl_news_df.at[i,'features'] = aapl_two_gram_features_array[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amzn_two_gram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "amzn_two_gram_features = amzn_two_gram_vectorizer.fit_transform(corpus_amzn)\n",
    "\n",
    "#printing first 30 feature words for visualization\n",
    "print(amzn_two_gram_vectorizer.get_feature_names()[:30])\n",
    "\n",
    "amzn_two_gram_features_array = amzn_two_gram_features.toarray()\n",
    "print(\"Amzn two-gram feature vector size: %d X %d\" % (len(amzn_two_gram_features_array), len(amzn_two_gram_features_array[0])))\n",
    "\n",
    "for i in range(len(amzn_two_gram_features_array)):\n",
    "    labelled_amzn_news_df.at[i,'features'] = amzn_two_gram_features_array[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save two-gram features labelled data\n",
    "\n",
    "with open('aapl_two_gram_features_labelled_df.pkl', 'wb') as f:\n",
    "    pickle.dump(labelled_aapl_news_df, f)\n",
    "with open('amzn_two_gram_features_labelled_df.pkl', 'wb') as f:\n",
    "    pickle.dump(labelled_amzn_news_df, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
