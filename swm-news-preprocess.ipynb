{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1616366345156,
     "user": {
      "displayName": "Abhishek Devasya Venkatramana",
      "photoUrl": "",
      "userId": "06922042641627792474"
     },
     "user_tz": 420
    },
    "id": "EW_KdqRdwAHy"
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import json\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1616364820543,
     "user": {
      "displayName": "Abhishek Devasya Venkatramana",
      "photoUrl": "",
      "userId": "06922042641627792474"
     },
     "user_tz": 420
    },
    "id": "esU05LDjwTQ5",
    "outputId": "280dccbe-1f17-47e4-826d-a28e45fd2a19"
   },
   "outputs": [],
   "source": [
    "NEWS_DIRECTORY = '/media/adv/Data/PROJECTS/CSE573-SWM/News'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/adv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/adv/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/adv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download libraries\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2yZGBQflwpNK"
   },
   "outputs": [],
   "source": [
    "def get_news_articles(path):\n",
    "    news_text = []\n",
    "    news_publish_time = []\n",
    "    news_source = []\n",
    "    domains_to_select = ['ae', 'au', 'bb', 'biz', 'ca', 'in', 'io', 'net', 'uk', 'com']\n",
    "    \n",
    "    \n",
    "    with os.scandir(path) as news_directories:\n",
    "        for directory in news_directories:\n",
    "            with os.scandir(os.path.join(path, directory.name)) as folder:\n",
    "                for article in folder:\n",
    "                    with open(os.path.join(path, directory.name, article.name), encoding='utf-8') as f:\n",
    "                        news_data = json.load(f)\n",
    "                    if 'site' in news_data['thread'] and news_data['thread']['site']:\n",
    "                        source = news_data['thread']['site'] \n",
    "                        domain = source.split('.')[-1]\n",
    "                        # Skip news from domain not in the list\n",
    "                        if domain not in domains_to_select:\n",
    "                            continue\n",
    "                        news_source.append(source)\n",
    "                    else:\n",
    "                        news_source.append(None)\n",
    "                    if 'published' in news_data and news_data['published']:\n",
    "                        news_publish_time.append(news_data['published'])\n",
    "                    else:\n",
    "                        news_publish_time.append(None)\n",
    "                    if 'text' in news_data and news_data['text']:\n",
    "                        news_text.append(news_data['text'])\n",
    "                    else:\n",
    "                        news_text.append(None)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': news_publish_time,\n",
    "        'text': news_text,\n",
    "        'source': news_source,\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "news_df = get_news_articles(NEWS_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2018-06-04T03:00:00.000+03:00</td>\n",
       "      <td>At its annual WWDC keynote on Monday, Apple In...</td>\n",
       "      <td>marketwatch.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2018-06-04T03:00:00.000+03:00</td>\n",
       "      <td>Amazon.com Inc.'s stock AMZN, +0.58% rallied 0...</td>\n",
       "      <td>marketwatch.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2018-06-04T15:20:00.000+03:00</td>\n",
       "      <td>New York (Reuters) - Technology stocks led the...</td>\n",
       "      <td>reuters.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2018-06-03T23:44:00.000+03:00</td>\n",
       "      <td>Orleans Capital Management Upped Its Nextera ...</td>\n",
       "      <td>mmahotstuff.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2018-06-04T04:48:00.000+03:00</td>\n",
       "      <td>The bears have a case.\\nWith minimal economic ...</td>\n",
       "      <td>thestreet.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73496</td>\n",
       "      <td>2019-02-07T12:30:00.000+02:00</td>\n",
       "      <td>Apple đồng ý trả hơn nửa tỷ USD tiền nợ thuế t...</td>\n",
       "      <td>vietgiaitri.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73497</td>\n",
       "      <td>2019-02-06T16:56:00.000+02:00</td>\n",
       "      <td>-=Tableau Software (DATA) reported earnings on...</td>\n",
       "      <td>tradewitheva.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73498</td>\n",
       "      <td>2019-02-07T14:43:00.000+02:00</td>\n",
       "      <td>You follow Analyst Blog - edit You follow Zack...</td>\n",
       "      <td>zacks.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73499</td>\n",
       "      <td>2019-02-07T12:32:00.000+02:00</td>\n",
       "      <td>Apple đồng ý trả hơn nửa tỷ USD tiền nợ thuế t...</td>\n",
       "      <td>vietgiaitri.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>2019-02-07T22:25:00.000+02:00</td>\n",
       "      <td>Apple (NASDAQ: AAPL ) has moved its modem chip...</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73501 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           timestamp  \\\n",
       "0      2018-06-04T03:00:00.000+03:00   \n",
       "1      2018-06-04T03:00:00.000+03:00   \n",
       "2      2018-06-04T15:20:00.000+03:00   \n",
       "3      2018-06-03T23:44:00.000+03:00   \n",
       "4      2018-06-04T04:48:00.000+03:00   \n",
       "...                              ...   \n",
       "73496  2019-02-07T12:30:00.000+02:00   \n",
       "73497  2019-02-06T16:56:00.000+02:00   \n",
       "73498  2019-02-07T14:43:00.000+02:00   \n",
       "73499  2019-02-07T12:32:00.000+02:00   \n",
       "73500  2019-02-07T22:25:00.000+02:00   \n",
       "\n",
       "                                                    text            source  \n",
       "0      At its annual WWDC keynote on Monday, Apple In...   marketwatch.com  \n",
       "1      Amazon.com Inc.'s stock AMZN, +0.58% rallied 0...   marketwatch.com  \n",
       "2      New York (Reuters) - Technology stocks led the...       reuters.com  \n",
       "3       Orleans Capital Management Upped Its Nextera ...   mmahotstuff.com  \n",
       "4      The bears have a case.\\nWith minimal economic ...     thestreet.com  \n",
       "...                                                  ...               ...  \n",
       "73496  Apple đồng ý trả hơn nửa tỷ USD tiền nợ thuế t...   vietgiaitri.com  \n",
       "73497  -=Tableau Software (DATA) reported earnings on...  tradewitheva.com  \n",
       "73498  You follow Analyst Blog - edit You follow Zack...         zacks.com  \n",
       "73499  Apple đồng ý trả hơn nửa tỷ USD tiền nợ thuế t...   vietgiaitri.com  \n",
       "73500  Apple (NASDAQ: AAPL ) has moved its modem chip...  seekingalpha.com  \n",
       "\n",
       "[73501 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Q6GUZqZi7M3u"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp    0\n",
       "text         0\n",
       "source       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timezone to UTC and drop the timezone\n",
    "news_df['timestamp'] = news_df['timestamp'].apply(lambda x: datetime.fromisoformat(x).astimezone(tz=timezone.utc))\n",
    "news_df['timestamp'] = news_df['timestamp'].dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_df['time'] = news_df['publish_timestamp'].apply(lambda x: x.time())\n",
    "# news_df['date'] = news_df['publish_timestamp'].apply(lambda x: x.date())\n",
    "news_df['text'] = news_df['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.to_csv('RawNewsData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'text', 'source'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['timestamp', 'source', 'sentences']\n",
    "processed_amzn_news_df = pd.DataFrame(columns=columns)\n",
    "processed_aapl_news_df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20236 73175\n"
     ]
    }
   ],
   "source": [
    "# Extract and separate sentences containing AAPL and AMZN\n",
    "for index, row in news_df.iterrows():\n",
    "    text = row['text']\n",
    "    aapl_sentences, amzn_sentences = [], []\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        if 'amazon' in sentence or 'amzn' in sentence:\n",
    "            amzn_sentences.append(sentence)\n",
    "        if 'apple' in sentence or 'aapl' in sentence:\n",
    "            aapl_sentences.append(sentence)\n",
    "    if aapl_sentences:\n",
    "        processed_aapl_news_df.loc[len(processed_aapl_news_df)] = [row['timestamp'], row['source'], aapl_sentences]\n",
    "    if amzn_sentences:\n",
    "        processed_amzn_news_df.loc[len(processed_amzn_news_df)] = [row['timestamp'], row['source'], amzn_sentences]\n",
    "        \n",
    "print(len(processed_amzn_news_df), len(processed_aapl_news_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del news_df\n",
    "processed_amzn_news_df.to_csv('AmznExtractedSentences.csv')\n",
    "processed_aapl_news_df.to_csv('AaplExtractedSentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2018-06-04 00:00:00</td>\n",
       "      <td>marketwatch.com</td>\n",
       "      <td>[amazon.com inc.'s stock amzn, +0.58% rallied ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2018-06-04 15:10:00</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>[aum of $66.4b\\n52-week performance vs. the s&amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2018-06-04 00:00:00</td>\n",
       "      <td>marketwatch.com</td>\n",
       "      <td>[the technology sector is riding its way into ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2018-06-04 18:46:00</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[elsewhere facebook (nasdaq:fb) dipped 1.03% a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2018-06-04 18:46:00</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[elsewhere facebook (nasdaq:fb) dipped 1.03% a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20231</td>\n",
       "      <td>2019-02-07 15:51:00</td>\n",
       "      <td>nasdaq.com</td>\n",
       "      <td>[you look at facebook, you look at amazon , ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20232</td>\n",
       "      <td>2019-02-07 15:17:00</td>\n",
       "      <td>marketwatch.com</td>\n",
       "      <td>[• momo crowd money flows are positive in amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20233</td>\n",
       "      <td>2019-02-07 04:10:00</td>\n",
       "      <td>marketwatch.com</td>\n",
       "      <td>[that performance was good enough to land atop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20234</td>\n",
       "      <td>2019-02-07 03:43:00</td>\n",
       "      <td>barrons.com</td>\n",
       "      <td>[sonos speakers also support amazon.com ’s (am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20235</td>\n",
       "      <td>2019-02-06 14:56:00</td>\n",
       "      <td>tradewitheva.com</td>\n",
       "      <td>[posted by eva s at 5:00 pm\\nhere are some of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20236 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp            source  \\\n",
       "0     2018-06-04 00:00:00   marketwatch.com   \n",
       "1     2018-06-04 15:10:00  seekingalpha.com   \n",
       "2     2018-06-04 00:00:00   marketwatch.com   \n",
       "3     2018-06-04 18:46:00         yahoo.com   \n",
       "4     2018-06-04 18:46:00         yahoo.com   \n",
       "...                   ...               ...   \n",
       "20231 2019-02-07 15:51:00        nasdaq.com   \n",
       "20232 2019-02-07 15:17:00   marketwatch.com   \n",
       "20233 2019-02-07 04:10:00   marketwatch.com   \n",
       "20234 2019-02-07 03:43:00       barrons.com   \n",
       "20235 2019-02-06 14:56:00  tradewitheva.com   \n",
       "\n",
       "                                               sentences  \n",
       "0      [amazon.com inc.'s stock amzn, +0.58% rallied ...  \n",
       "1      [aum of $66.4b\\n52-week performance vs. the s&...  \n",
       "2      [the technology sector is riding its way into ...  \n",
       "3      [elsewhere facebook (nasdaq:fb) dipped 1.03% a...  \n",
       "4      [elsewhere facebook (nasdaq:fb) dipped 1.03% a...  \n",
       "...                                                  ...  \n",
       "20231  [you look at facebook, you look at amazon , ap...  \n",
       "20232  [• momo crowd money flows are positive in amaz...  \n",
       "20233  [that performance was good enough to land atop...  \n",
       "20234  [sonos speakers also support amazon.com ’s (am...  \n",
       "20235  [posted by eva s at 5:00 pm\\nhere are some of ...  \n",
       "\n",
       "[20236 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_amzn_news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_words(input_words):\n",
    "    from nltk.corpus import words\n",
    "    \n",
    "    # Remove all non-ascii words\n",
    "    processed_words = [w for w in input_words if w.isascii()]\n",
    "    \n",
    "    # Remove punctuation words\n",
    "    tr_dict = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    processed_words = [w.translate(tr_dict) for w in processed_words if w]\n",
    "    \n",
    "    # Remove links\n",
    "    final_words = []\n",
    "    for word in processed_words:\n",
    "        if not re.match('[www]', word):\n",
    "            final_words.append(word)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    processed_words = [w for w in final_words if w not in stop_words]\n",
    "    \n",
    "    # Stem words and return unique words\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    seen = set()\n",
    "    processed_words = [stemmer.stem(word) for word in processed_words if word]\n",
    "    processed_words = [x for x in processed_words if not (x in seen or seen.add(x))]\n",
    "    del seen\n",
    "    \n",
    "    # Keep only words from English dictionary\n",
    "    english_words = set([w.lower() for w in words.words()])\n",
    "    processed_words = [w for w in processed_words if w in english_words]\n",
    "    \n",
    "    return processed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing 20236 records\n",
      "Completed 0 rows\n",
      "Completed 500 rows\n",
      "Completed 1000 rows\n",
      "Completed 1500 rows\n",
      "Completed 2000 rows\n",
      "Completed 2500 rows\n",
      "Completed 3000 rows\n",
      "Completed 3500 rows\n",
      "Completed 4000 rows\n",
      "Completed 4500 rows\n",
      "Completed 5000 rows\n",
      "Completed 5500 rows\n",
      "Completed 6000 rows\n",
      "Completed 6500 rows\n",
      "Completed 7000 rows\n",
      "Completed 7500 rows\n",
      "Completed 8000 rows\n",
      "Completed 8500 rows\n",
      "Completed 9000 rows\n",
      "Completed 9500 rows\n",
      "Completed 10000 rows\n",
      "Completed 10500 rows\n",
      "Completed 11000 rows\n",
      "Completed 11500 rows\n",
      "Completed 12000 rows\n",
      "Completed 12500 rows\n",
      "Completed 13000 rows\n",
      "Completed 13500 rows\n",
      "Completed 14000 rows\n",
      "Completed 14500 rows\n",
      "Completed 15000 rows\n",
      "Completed 15500 rows\n",
      "Completed 16000 rows\n",
      "Completed 16500 rows\n",
      "Completed 17000 rows\n",
      "Completed 17500 rows\n",
      "Completed 18000 rows\n",
      "Completed 18500 rows\n",
      "Completed 19000 rows\n",
      "Completed 19500 rows\n",
      "Completed 20000 rows\n",
      "\n",
      "\n",
      "Processing 73175 records\n",
      "Completed 0 rows\n",
      "Completed 500 rows\n",
      "Completed 1000 rows\n",
      "Completed 1500 rows\n",
      "Completed 2000 rows\n",
      "Completed 2500 rows\n",
      "Completed 3000 rows\n",
      "Completed 3500 rows\n",
      "Completed 4000 rows\n",
      "Completed 4500 rows\n",
      "Completed 5000 rows\n",
      "Completed 5500 rows\n",
      "Completed 6000 rows\n",
      "Completed 6500 rows\n",
      "Completed 7000 rows\n",
      "Completed 7500 rows\n",
      "Completed 8000 rows\n",
      "Completed 8500 rows\n",
      "Completed 9000 rows\n",
      "Completed 9500 rows\n",
      "Completed 10000 rows\n",
      "Completed 10500 rows\n",
      "Completed 11000 rows\n",
      "Completed 11500 rows\n",
      "Completed 12000 rows\n",
      "Completed 12500 rows\n",
      "Completed 13000 rows\n",
      "Completed 13500 rows\n",
      "Completed 14000 rows\n",
      "Completed 14500 rows\n",
      "Completed 15000 rows\n",
      "Completed 15500 rows\n",
      "Completed 16000 rows\n",
      "Completed 16500 rows\n",
      "Completed 17000 rows\n",
      "Completed 17500 rows\n",
      "Completed 18000 rows\n",
      "Completed 18500 rows\n",
      "Completed 19000 rows\n",
      "Completed 19500 rows\n",
      "Completed 20000 rows\n",
      "Completed 20500 rows\n",
      "Completed 21000 rows\n",
      "Completed 21500 rows\n",
      "Completed 22000 rows\n",
      "Completed 22500 rows\n",
      "Completed 23000 rows\n",
      "Completed 23500 rows\n",
      "Completed 24000 rows\n",
      "Completed 24500 rows\n",
      "Completed 25000 rows\n",
      "Completed 25500 rows\n",
      "Completed 26000 rows\n",
      "Completed 26500 rows\n",
      "Completed 27000 rows\n",
      "Completed 27500 rows\n",
      "Completed 28000 rows\n",
      "Completed 28500 rows\n",
      "Completed 29000 rows\n",
      "Completed 29500 rows\n",
      "Completed 30000 rows\n",
      "Completed 30500 rows\n",
      "Completed 31000 rows\n",
      "Completed 31500 rows\n",
      "Completed 32000 rows\n",
      "Completed 32500 rows\n",
      "Completed 33000 rows\n",
      "Completed 33500 rows\n",
      "Completed 34000 rows\n",
      "Completed 34500 rows\n",
      "Completed 35000 rows\n",
      "Completed 35500 rows\n",
      "Completed 36000 rows\n",
      "Completed 36500 rows\n",
      "Completed 37000 rows\n",
      "Completed 37500 rows\n",
      "Completed 38000 rows\n",
      "Completed 38500 rows\n",
      "Completed 39000 rows\n",
      "Completed 39500 rows\n",
      "Completed 40000 rows\n",
      "Completed 40500 rows\n",
      "Completed 41000 rows\n",
      "Completed 41500 rows\n",
      "Completed 42000 rows\n",
      "Completed 42500 rows\n",
      "Completed 43000 rows\n",
      "Completed 43500 rows\n",
      "Completed 44000 rows\n",
      "Completed 44500 rows\n",
      "Completed 45000 rows\n",
      "Completed 45500 rows\n",
      "Completed 46000 rows\n",
      "Completed 46500 rows\n",
      "Completed 47000 rows\n",
      "Completed 47500 rows\n",
      "Completed 48000 rows\n",
      "Completed 48500 rows\n",
      "Completed 49000 rows\n",
      "Completed 49500 rows\n",
      "Completed 50000 rows\n",
      "Completed 50500 rows\n",
      "Completed 51000 rows\n",
      "Completed 51500 rows\n",
      "Completed 52000 rows\n",
      "Completed 52500 rows\n",
      "Completed 53000 rows\n",
      "Completed 53500 rows\n",
      "Completed 54000 rows\n",
      "Completed 54500 rows\n",
      "Completed 55000 rows\n",
      "Completed 55500 rows\n",
      "Completed 56000 rows\n",
      "Completed 56500 rows\n",
      "Completed 57000 rows\n",
      "Completed 57500 rows\n",
      "Completed 58000 rows\n",
      "Completed 58500 rows\n",
      "Completed 59000 rows\n",
      "Completed 59500 rows\n",
      "Completed 60000 rows\n",
      "Completed 60500 rows\n",
      "Completed 61000 rows\n",
      "Completed 61500 rows\n",
      "Completed 62000 rows\n",
      "Completed 62500 rows\n",
      "Completed 63000 rows\n",
      "Completed 63500 rows\n",
      "Completed 64000 rows\n",
      "Completed 64500 rows\n",
      "Completed 65000 rows\n",
      "Completed 65500 rows\n",
      "Completed 66000 rows\n",
      "Completed 66500 rows\n",
      "Completed 67000 rows\n",
      "Completed 67500 rows\n",
      "Completed 68000 rows\n",
      "Completed 68500 rows\n",
      "Completed 69000 rows\n",
      "Completed 69500 rows\n",
      "Completed 70000 rows\n",
      "Completed 70500 rows\n",
      "Completed 71000 rows\n",
      "Completed 71500 rows\n",
      "Completed 72000 rows\n",
      "Completed 72500 rows\n",
      "Completed 73000 rows\n"
     ]
    }
   ],
   "source": [
    "tokenized_df_columns = ['timestamp', 'source', 'tokens']\n",
    "tokenized_amzn_news_df = pd.DataFrame(columns=tokenized_df_columns)\n",
    "tokenized_aapl_news_df = pd.DataFrame(columns=tokenized_df_columns)\n",
    "\n",
    "print(\"\\n\\nProcessing %d records\" % len(processed_amzn_news_df))\n",
    "for index, row in processed_amzn_news_df.iterrows():\n",
    "    # This break is only for testing purpose\n",
    "#     if index >= 1000:\n",
    "#         break\n",
    "        \n",
    "    if index % 500 == 0:\n",
    "        print(\"Completed %d rows\" % index)\n",
    "    token_words = []\n",
    "    for sentence in row['sentences']:\n",
    "        token_words.extend(nltk.wordpunct_tokenize(sentence))\n",
    "    token_words = extract_words(token_words)\n",
    "    tokenized_amzn_news_df.loc[index] = [\n",
    "        processed_amzn_news_df.loc[index]['timestamp'],\n",
    "        processed_amzn_news_df.loc[index]['source'], \n",
    "        token_words\n",
    "    ]\n",
    "tokenized_amzn_news_df.to_csv('AmznExtractedTokens.csv')\n",
    "\n",
    "\n",
    "print(\"\\n\\nProcessing %d records\" % len(processed_aapl_news_df))\n",
    "for index, row in processed_aapl_news_df.iterrows():\n",
    "    # This break is only for testing purpose\n",
    "#     if index >= 1000:\n",
    "#         break\n",
    "    if index % 500 == 0:\n",
    "        print(\"Completed %d rows\" % index)\n",
    "    token_words = []\n",
    "    for sentence in row['sentences']:\n",
    "        token_words.extend(nltk.wordpunct_tokenize(sentence))\n",
    "    token_words = extract_words(token_words)\n",
    "    tokenized_aapl_news_df.loc[index] = [\n",
    "        processed_aapl_news_df.loc[index]['timestamp'], \n",
    "        processed_aapl_news_df.loc[index]['source'],\n",
    "        token_words\n",
    "    ]\n",
    "tokenized_aapl_news_df.to_csv('AaplExtractedTokens.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframes based on timestamps\n",
    "# tokenized_amzn_news_df.sort_values(['day', 'time'], axis=0, ascending=(True, True), inplace=True)\n",
    "# tokenized_aapl_news_df.sort_values(['day', 'time'], axis=0, ascending=(True, True), inplace=True)\n",
    "tokenized_amzn_news_df.sort_values(['timestamp'], axis=0, ascending=True, inplace=True)\n",
    "tokenized_aapl_news_df.sort_values(['timestamp'], axis=0, ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('AmznExtractedTokens.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenized_amzn_news_df, f)\n",
    "with open('AaplExtractedTokens.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenized_aapl_news_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toward',\n",
       " 'cap',\n",
       " 'target',\n",
       " 'e',\n",
       " 'record',\n",
       " 'climb',\n",
       " 'billion',\n",
       " 'stock',\n",
       " 'boost',\n",
       " 'run',\n",
       " 'report',\n",
       " 'month',\n",
       " 'current',\n",
       " 'mark',\n",
       " 'quarter',\n",
       " 'second',\n",
       " 'outstand',\n",
       " 'humphrey',\n",
       " 'amazon',\n",
       " 'make',\n",
       " 'monday',\n",
       " 'million',\n",
       " 'result',\n",
       " 'dow',\n",
       " 'streak',\n",
       " 'price',\n",
       " 'giant',\n",
       " 'past',\n",
       " 'lift',\n",
       " 'reach',\n",
       " 'april',\n",
       " 'share',\n",
       " 'base',\n",
       " 'sixth',\n",
       " 'straight',\n",
       " 'market',\n",
       " 'morn',\n",
       " 'behind',\n",
       " 'first',\n",
       " 'three',\n",
       " 'u',\n",
       " 'help',\n",
       " 'close',\n",
       " 'enough',\n",
       " 'trade']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_amzn_news_df['tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP28kHTgHIiEimh6OnqOEzf",
   "collapsed_sections": [],
   "name": "swm-news-preprocess.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
