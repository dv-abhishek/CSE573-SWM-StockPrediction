{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "nltk.download(\"popular\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tfidf = TfidfTransformer(smooth_idf=True,use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized news data\n",
    "\n",
    "with open('AaplLabelledNewsData-OrderPreserved.pkl', 'rb') as f:\n",
    "    labelled_aapl_news_df = pickle.load(f)\n",
    "    \n",
    "with open('AmznLabelledNewsData-OrderPreserved.pkl', 'rb') as f:\n",
    "    labelled_amzn_news_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_timestamp</th>\n",
       "      <th>stock_timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-07 20:00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>businesswire.com</td>\n",
       "      <td>[nape, summit, also, annual, luncheon, tom, sp...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-08 21:37:00</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[huge, x, comeback, tour, follow, note, big, y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-12 01:55:00</td>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[browser, os, ad, server, make, first, move, c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-12 22:10:00</td>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>investingnews.com</td>\n",
       "      <td>[humanitarian, place, conflict, free, region]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-14 11:42:00</td>\n",
       "      <td>2017-12-15</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>[prefer, invest]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       news_timestamp stock_timestamp             source  \\\n",
       "0 2017-12-07 20:00:00      2017-12-08   businesswire.com   \n",
       "1 2017-12-08 21:37:00      2017-12-11          yahoo.com   \n",
       "2 2017-12-12 01:55:00      2017-12-13          yahoo.com   \n",
       "3 2017-12-12 22:10:00      2017-12-13  investingnews.com   \n",
       "4 2017-12-14 11:42:00      2017-12-15   seekingalpha.com   \n",
       "\n",
       "                                              tokens label  \n",
       "0  [nape, summit, also, annual, luncheon, tom, sp...    -1  \n",
       "1  [huge, x, comeback, tour, follow, note, big, y...     1  \n",
       "2  [browser, os, ad, server, make, first, move, c...     1  \n",
       "3      [humanitarian, place, conflict, free, region]     1  \n",
       "4                                   [prefer, invest]     1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize data\n",
    "labelled_aapl_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_timestamp</th>\n",
       "      <th>stock_timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19911</th>\n",
       "      <td>2019-01-31 21:42:00</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>nasdaq.com</td>\n",
       "      <td>[also, prove, reach, mani, entertain, age, soo...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19912</th>\n",
       "      <td>2019-01-31 22:03:00</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>makemefeed.com</td>\n",
       "      <td>[read, amazon, general, avail, publish, cost, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19913</th>\n",
       "      <td>2019-01-31 22:05:00</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[direct, cloud, amazon, unit, driven, match, l...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19914</th>\n",
       "      <td>2019-01-31 23:24:00</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>marketwatch.com</td>\n",
       "      <td>[earn, up, say, spend, strong, amazon, real, t...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19915</th>\n",
       "      <td>2019-01-31 23:31:00</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>marketwatch.com</td>\n",
       "      <td>[earn, up, say, spend, strong, amazon, real, t...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           news_timestamp stock_timestamp           source  \\\n",
       "19911 2019-01-31 21:42:00      2019-02-01       nasdaq.com   \n",
       "19912 2019-01-31 22:03:00      2019-02-01   makemefeed.com   \n",
       "19913 2019-01-31 22:05:00      2019-02-01        yahoo.com   \n",
       "19914 2019-01-31 23:24:00      2019-02-01  marketwatch.com   \n",
       "19915 2019-01-31 23:31:00      2019-02-01  marketwatch.com   \n",
       "\n",
       "                                                  tokens label  \n",
       "19911  [also, prove, reach, mani, entertain, age, soo...    -1  \n",
       "19912  [read, amazon, general, avail, publish, cost, ...    -1  \n",
       "19913  [direct, cloud, amazon, unit, driven, match, l...    -1  \n",
       "19914  [earn, up, say, spend, strong, amazon, real, t...    -1  \n",
       "19915  [earn, up, say, spend, strong, amazon, real, t...    -1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_amzn_news_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple dataframe size is: 71941\n",
      "amazon dataframe size is: 19916\n",
      "mixed dataframe size is: 91857\n"
     ]
    }
   ],
   "source": [
    "# combine both dataframes\n",
    "frames = [labelled_aapl_news_df, labelled_amzn_news_df]\n",
    "aapl_amzn_mixed_df = pd.concat(frames, axis=0, ignore_index=True)\n",
    "print('apple dataframe size is: %d' % len(labelled_aapl_news_df))\n",
    "print('amazon dataframe size is: %d' % len(labelled_amzn_news_df))\n",
    "print('mixed dataframe size is: %d' % len(aapl_amzn_mixed_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_timestamp</th>\n",
       "      <th>stock_timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-07 20:00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>businesswire.com</td>\n",
       "      <td>[nape, summit, also, annual, luncheon, tom, sp...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-08 21:37:00</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[huge, x, comeback, tour, follow, note, big, y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-12 01:55:00</td>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[browser, os, ad, server, make, first, move, c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-12 22:10:00</td>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>investingnews.com</td>\n",
       "      <td>[humanitarian, place, conflict, free, region]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-14 11:42:00</td>\n",
       "      <td>2017-12-15</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>[prefer, invest]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       news_timestamp stock_timestamp             source  \\\n",
       "0 2017-12-07 20:00:00      2017-12-08   businesswire.com   \n",
       "1 2017-12-08 21:37:00      2017-12-11          yahoo.com   \n",
       "2 2017-12-12 01:55:00      2017-12-13          yahoo.com   \n",
       "3 2017-12-12 22:10:00      2017-12-13  investingnews.com   \n",
       "4 2017-12-14 11:42:00      2017-12-15   seekingalpha.com   \n",
       "\n",
       "                                              tokens label  \n",
       "0  [nape, summit, also, annual, luncheon, tom, sp...    -1  \n",
       "1  [huge, x, comeback, tour, follow, note, big, y...     1  \n",
       "2  [browser, os, ad, server, make, first, move, c...     1  \n",
       "3      [humanitarian, place, conflict, free, region]     1  \n",
       "4                                   [prefer, invest]     1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize mixed frame data\n",
    "aapl_amzn_mixed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_timestamp</th>\n",
       "      <th>stock_timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91852</th>\n",
       "      <td>2019-01-31 21:42:00</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>nasdaq.com</td>\n",
       "      <td>[also, prove, reach, mani, entertain, age, soo...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91853</th>\n",
       "      <td>2019-01-31 22:03:00</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>makemefeed.com</td>\n",
       "      <td>[read, amazon, general, avail, publish, cost, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91854</th>\n",
       "      <td>2019-01-31 22:05:00</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[direct, cloud, amazon, unit, driven, match, l...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91855</th>\n",
       "      <td>2019-01-31 23:24:00</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>marketwatch.com</td>\n",
       "      <td>[earn, up, say, spend, strong, amazon, real, t...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91856</th>\n",
       "      <td>2019-01-31 23:31:00</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>marketwatch.com</td>\n",
       "      <td>[earn, up, say, spend, strong, amazon, real, t...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           news_timestamp stock_timestamp           source  \\\n",
       "91852 2019-01-31 21:42:00      2019-02-01       nasdaq.com   \n",
       "91853 2019-01-31 22:03:00      2019-02-01   makemefeed.com   \n",
       "91854 2019-01-31 22:05:00      2019-02-01        yahoo.com   \n",
       "91855 2019-01-31 23:24:00      2019-02-01  marketwatch.com   \n",
       "91856 2019-01-31 23:31:00      2019-02-01  marketwatch.com   \n",
       "\n",
       "                                                  tokens label  \n",
       "91852  [also, prove, reach, mani, entertain, age, soo...    -1  \n",
       "91853  [read, amazon, general, avail, publish, cost, ...    -1  \n",
       "91854  [direct, cloud, amazon, unit, driven, match, l...    -1  \n",
       "91855  [earn, up, say, spend, strong, amazon, real, t...    -1  \n",
       "91856  [earn, up, say, spend, strong, amazon, real, t...    -1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize mixed frame data\n",
    "aapl_amzn_mixed_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 rows for mixed corpus\n",
      "Completed 10000 rows for mixed corpus\n",
      "Completed 20000 rows for mixed corpus\n",
      "Completed 30000 rows for mixed corpus\n",
      "Completed 40000 rows for mixed corpus\n",
      "Completed 50000 rows for mixed corpus\n",
      "Completed 60000 rows for mixed corpus\n",
      "Completed 70000 rows for mixed corpus\n",
      "Completed 80000 rows for mixed corpus\n",
      "Completed 90000 rows for mixed corpus\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "#can try to vectorize this\n",
    "for index, row in aapl_amzn_mixed_df.iterrows():\n",
    "    corpus.append(' '.join(row['tokens']))\n",
    "    if index % 10000 == 0:\n",
    "        print(\"Completed %d rows for mixed corpus\" % index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nape summit also annual luncheon tom speaker new job fair upstream profession seminar present organ seg intern theater run two day prospect preview near exhibit space',\n",
       " 'huge x comeback tour follow note big year best phone money one buy premium start cool per month depend plan put reach mani spot face id facial fantast dual len camera help make kind need time analyst question sure loss home button disappear could full screen handset turn user slick touch control even tire go high power second plus lover impress display got great two mean much fan',\n",
       " 'browser os ad server make first move competitor like follow lead context ecosystem rife fraud publish ought much kind low rent unit reader rob hacker might target plan kill net neutral affect still use x face id even though say beat big media serf land tech giant sold buy twitter',\n",
       " 'humanitarian place conflict free region',\n",
       " 'prefer invest']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print sample items to ensure update\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['also prove reach mani entertain age soon non ad support stream platform like amazon enough dis remain digit juggernaut along year come',\n",
       " 'read amazon general avail publish cost product use candid sent letter fund mogul nascent ad bigger ever start make inroad big brand made billion sale earn fourth quarter said call grab market critic link embed develop kit n',\n",
       " 'direct cloud amazon unit driven match last quarter growth aw',\n",
       " 'earn up say spend strong amazon real test analyst maria forecast like bake lot given',\n",
       " 'earn up say spend strong amazon real test analyst maria forecast like bake lot given']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['access', 'accord', 'account', 'acronym', 'across', 'act', 'action', 'actual', 'ad', 'add', 'address', 'adjust', 'admit', 'adopt', 'advisor', 'affect', 'afford', 'afternoon', 'aggress', 'ago', 'agre', 'agreement', 'ahead', 'ai', 'aim', 'air', 'alert', 'allow', 'almost', 'along']\n",
      "one-gram feature vector size: 91857 X 1039\n"
     ]
    }
   ],
   "source": [
    "one_gram_vectorizer = CountVectorizer(max_df=0.9, min_df=0.005, stop_words=stop_words, ngram_range=(1,1))\n",
    "one_gram_features = one_gram_vectorizer.fit_transform(corpus)\n",
    "\n",
    "#printing first 30 feature words for visualization\n",
    "print(one_gram_vectorizer.get_feature_names()[:30])\n",
    "\n",
    "aapl_amzn_mixed_df = aapl_amzn_mixed_df.rename(columns={'tokens': 'features'})\n",
    "\n",
    "one_gram_features_array = tfidf.fit_transform(one_gram_features).toarray()\n",
    "print(\"one-gram feature vector size: %d X %d\" % (len(one_gram_features_array), len(one_gram_features_array[0])))\n",
    "\n",
    "for i in range(len(one_gram_features_array)):\n",
    "    aapl_amzn_mixed_df.at[i,'features'] = one_gram_features_array[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_timestamp</th>\n",
       "      <th>stock_timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-07 20:00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>businesswire.com</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-08 21:37:00</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-12 01:55:00</td>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.156...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-12 22:10:00</td>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>investingnews.com</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-14 11:42:00</td>\n",
       "      <td>2017-12-15</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       news_timestamp stock_timestamp             source  \\\n",
       "0 2017-12-07 20:00:00      2017-12-08   businesswire.com   \n",
       "1 2017-12-08 21:37:00      2017-12-11          yahoo.com   \n",
       "2 2017-12-12 01:55:00      2017-12-13          yahoo.com   \n",
       "3 2017-12-12 22:10:00      2017-12-13  investingnews.com   \n",
       "4 2017-12-14 11:42:00      2017-12-15   seekingalpha.com   \n",
       "\n",
       "                                            features label  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    -1  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     1  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.156...     1  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     1  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print sample frames\n",
    "aapl_amzn_mixed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save one-gram features labelled data\n",
    "\n",
    "with open('mixed_one_gram_features_labelled_df.pkl', 'wb') as f:\n",
    "    pickle.dump(aapl_amzn_mixed_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['access design', 'accord data', 'accord file', 'accord maintain', 'accord recent', 'accord report', 'accord stock', 'account invest', 'account portfolio', 'action alert', 'actual previous', 'ad stake', 'address free', 'address latest', 'advisor final', 'advisor new', 'advisor return', 'alert news', 'alert number', 'alert open', 'alert plus', 'alert sever', 'alert trade', 'allow user', 'alphabet amazon', 'also ad', 'also bought', 'also hold', 'also made', 'also recent']\n",
      "['two day', 'two low', 'two year', 'unit sale', 'unit state', 'unveil new', 'uptrend among', 'uptrend analyst', 'uptrend investor', 'uptrend news', 'uptrend recent', 'us long', 'version access', 'version content', 'version read', 'version view', 'view design', 'visit latest', 'vital data', 'year ago', 'year analyst', 'year board', 'year date', 'year dividend', 'year low', 'year new', 'year news', 'year per', 'year profit', 'york time']\n",
      "two-gram feature vector size: 91857 X 817\n"
     ]
    }
   ],
   "source": [
    "#extracting two-gram features now\n",
    "\n",
    "two_gram_vectorizer = CountVectorizer(max_df=0.9, min_df=0.003, stop_words=stop_words, ngram_range=(2,2))\n",
    "two_gram_features = two_gram_vectorizer.fit_transform(corpus)\n",
    "\n",
    "#printing first 30 feature words for visualization\n",
    "print(two_gram_vectorizer.get_feature_names()[:30])\n",
    "print(two_gram_vectorizer.get_feature_names()[-30:])\n",
    "\n",
    "two_gram_features_array=tfidf.fit_transform(two_gram_features).toarray()\n",
    "\n",
    "print(\"two-gram feature vector size: %d X %d\" % (len(two_gram_features_array), len(two_gram_features_array[0])))\n",
    "\n",
    "for i in range(len(two_gram_features_array)):\n",
    "    aapl_amzn_mixed_df.at[i,'features'] = two_gram_features_array[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_timestamp</th>\n",
       "      <th>stock_timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-07 20:00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>businesswire.com</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-08 21:37:00</td>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-12 01:55:00</td>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-12 22:10:00</td>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>investingnews.com</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-14 11:42:00</td>\n",
       "      <td>2017-12-15</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       news_timestamp stock_timestamp             source  \\\n",
       "0 2017-12-07 20:00:00      2017-12-08   businesswire.com   \n",
       "1 2017-12-08 21:37:00      2017-12-11          yahoo.com   \n",
       "2 2017-12-12 01:55:00      2017-12-13          yahoo.com   \n",
       "3 2017-12-12 22:10:00      2017-12-13  investingnews.com   \n",
       "4 2017-12-14 11:42:00      2017-12-15   seekingalpha.com   \n",
       "\n",
       "                                            features label  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    -1  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     1  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     1  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     1  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print sample frames\n",
    "aapl_amzn_mixed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save two-gram features labelled data for Apple\n",
    "\n",
    "with open('mixed_two_gram_features_labelled_df.pkl', 'wb') as f:\n",
    "    pickle.dump(aapl_amzn_mixed_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save two-gram features labelled data for Amazon\n",
    "\n",
    "with open('amzn_two_gram_features_labelled_df.pkl', 'wb') as f:\n",
    "    pickle.dump(labelled_amzn_news_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
