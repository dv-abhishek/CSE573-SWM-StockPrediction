{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Naveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download(\"popular\")\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized news data\n",
    "\n",
    "with open('AaplLabelledNewsData-four-hour.pkl', 'rb') as f:\n",
    "    labelled_aapl_news_df = pickle.load(f)\n",
    "    \n",
    "with open('AmznLabelledNewsData-four-hour.pkl', 'rb') as f:\n",
    "    labelled_amzn_news_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_timestamp</th>\n",
       "      <th>stock_timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-07 20:00:00</td>\n",
       "      <td>2017-12-07 21:30:00</td>\n",
       "      <td>businesswire.com</td>\n",
       "      <td>[nape, luncheon, organ, fair, near, profession...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-08 21:37:00</td>\n",
       "      <td>2017-12-11 13:30:00</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[buy, loss, got, question, follow, go, len, pl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-12 01:55:00</td>\n",
       "      <td>2017-12-12 13:30:00</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[reader, might, os, face, target, low, affect,...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-12 22:10:00</td>\n",
       "      <td>2017-12-13 13:30:00</td>\n",
       "      <td>investingnews.com</td>\n",
       "      <td>[conflict, region, place, humanitarian, free]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-14 11:42:00</td>\n",
       "      <td>2017-12-14 13:30:00</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>[invest, prefer]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       news_timestamp     stock_timestamp             source  \\\n",
       "0 2017-12-07 20:00:00 2017-12-07 21:30:00   businesswire.com   \n",
       "1 2017-12-08 21:37:00 2017-12-11 13:30:00          yahoo.com   \n",
       "2 2017-12-12 01:55:00 2017-12-12 13:30:00          yahoo.com   \n",
       "3 2017-12-12 22:10:00 2017-12-13 13:30:00  investingnews.com   \n",
       "4 2017-12-14 11:42:00 2017-12-14 13:30:00   seekingalpha.com   \n",
       "\n",
       "                                              tokens label  \n",
       "0  [nape, luncheon, organ, fair, near, profession...     1  \n",
       "1  [buy, loss, got, question, follow, go, len, pl...     1  \n",
       "2  [reader, might, os, face, target, low, affect,...    -1  \n",
       "3      [conflict, region, place, humanitarian, free]     1  \n",
       "4                                   [invest, prefer]    -1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize data\n",
    "labelled_aapl_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_timestamp</th>\n",
       "      <th>stock_timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-15 12:04:00</td>\n",
       "      <td>2017-12-15 13:30:00</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>[prime, e, support, ago, video, amazon, soon, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-21 12:04:00</td>\n",
       "      <td>2017-12-21 13:30:00</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>[public, futurist, fashion, yet, ago, amazon, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-21 12:27:00</td>\n",
       "      <td>2017-12-21 13:30:00</td>\n",
       "      <td>thestreet.com</td>\n",
       "      <td>[fit, stock, alphabet, amazon, intern, drug, f...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-22 00:00:00</td>\n",
       "      <td>2017-12-22 13:30:00</td>\n",
       "      <td>investors.com</td>\n",
       "      <td>[amazon, cloud, china]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-22 20:57:00</td>\n",
       "      <td>2017-12-26 13:30:00</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[bring, built, stream, soon, stop, play, go, p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       news_timestamp     stock_timestamp            source  \\\n",
       "0 2017-12-15 12:04:00 2017-12-15 13:30:00  seekingalpha.com   \n",
       "1 2017-12-21 12:04:00 2017-12-21 13:30:00  seekingalpha.com   \n",
       "2 2017-12-21 12:27:00 2017-12-21 13:30:00     thestreet.com   \n",
       "3 2017-12-22 00:00:00 2017-12-22 13:30:00     investors.com   \n",
       "4 2017-12-22 20:57:00 2017-12-26 13:30:00         yahoo.com   \n",
       "\n",
       "                                              tokens label  \n",
       "0  [prime, e, support, ago, video, amazon, soon, ...     1  \n",
       "1  [public, futurist, fashion, yet, ago, amazon, ...    -1  \n",
       "2  [fit, stock, alphabet, amazon, intern, drug, f...    -1  \n",
       "3                             [amazon, cloud, china]    -1  \n",
       "4  [bring, built, stream, soon, stop, play, go, p...     1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_amzn_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 rows for apple\n",
      "Completed 10000 rows for apple\n",
      "Completed 20000 rows for apple\n",
      "Completed 30000 rows for apple\n",
      "Completed 40000 rows for apple\n",
      "Completed 50000 rows for apple\n",
      "Completed 60000 rows for apple\n",
      "Completed 70000 rows for apple\n",
      "Completed 0 rows for amazon\n",
      "Completed 5000 rows for amazon\n",
      "Completed 10000 rows for amazon\n",
      "Completed 15000 rows for amazon\n"
     ]
    }
   ],
   "source": [
    "columns = ['merged_tokens']\n",
    "corpus_aapl = []\n",
    "corpus_amzn = []\n",
    "\n",
    "#can try to vectorize this\n",
    "for index, row in labelled_aapl_news_df.iterrows():\n",
    "    corpus_aapl.append(' '.join(row['tokens']))\n",
    "    if index % 10000 == 0:\n",
    "        print(\"Completed %d rows for apple\" % index)\n",
    "    \n",
    "for index, row in labelled_amzn_news_df.iterrows():\n",
    "    corpus_amzn.append(' '.join(row['tokens']))\n",
    "    if index % 5000 == 0:\n",
    "        print(\"Completed %d rows for amazon\" % index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nape luncheon organ fair near profession theater speaker seg new upstream exhibit present prospect space annual run tom seminar job intern also two day preview summit',\n",
       " 'buy loss got question follow go len plus big mani year id month sure user comeback second great make huge camera money lover home x best plan much two disappear facial face display phone touch depend note fan premium put analyst control kind screen slick time full reach cool dual one high spot handset could tour turn need impress button fantast per mean power help even tire start',\n",
       " 'reader might os face target low affect rob publish buy first context beat make ought media server ecosystem kill say giant browser still land follow use unit sold twitter rife competitor ad though kind fraud big even hacker lead rent x net id plan much move like neutral tech serf',\n",
       " 'conflict region place humanitarian free',\n",
       " 'invest prefer']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print sample items to ensure update\n",
    "corpus_aapl[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prime e support ago video amazon soon pull sell platform year two',\n",
       " 'public futurist fashion yet ago amazon open go line store cashier similar year',\n",
       " 'fit stock alphabet amazon intern drug fang',\n",
       " 'amazon cloud china',\n",
       " 'bring built stream soon stop play go plus current port content prime cut video amazon look show exist come expect back home product get iter like away jan diamond fire final thank access pull price smart spec playground control forth stick bummer cast latest interact said usual might assist ultra offer hope box shape also']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_amzn[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['access', 'accord', 'account', 'across', 'action', 'actual', 'ad', 'add', 'address', 'adjust', 'advisor', 'affect', 'afford', 'afternoon', 'aggress', 'ago', 'ahead', 'air', 'alert', 'allow', 'almost', 'along', 'alphabet', 'also', 'although', 'alway', 'amazon', 'america', 'american', 'amid']\n",
      "Aapl one-gram feature vector size: 72122 X 702\n"
     ]
    }
   ],
   "source": [
    "aapl_one_gram_vectorizer = CountVectorizer(max_df=0.9, min_df=0.01, stop_words=stop_words, ngram_range=(1,1))\n",
    "aapl_one_gram_features = aapl_one_gram_vectorizer.fit_transform(corpus_aapl)\n",
    "\n",
    "#printing first 30 feature words for visualization\n",
    "print(aapl_one_gram_vectorizer.get_feature_names()[:30])\n",
    "\n",
    "labelled_aapl_news_df = labelled_aapl_news_df.rename(columns={'tokens': 'features'})\n",
    "\n",
    "aapl_one_gram_features_array = aapl_one_gram_features.toarray()\n",
    "print(\"Aapl one-gram feature vector size: %d X %d\" % (len(aapl_one_gram_features_array), len(aapl_one_gram_features_array[0])))\n",
    "\n",
    "for i in range(len(aapl_one_gram_features_array)):\n",
    "    labelled_aapl_news_df.at[i,'features'] = aapl_one_gram_features_array[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_timestamp</th>\n",
       "      <th>stock_timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-07 20:00:00</td>\n",
       "      <td>2017-12-07 21:30:00</td>\n",
       "      <td>businesswire.com</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-08 21:37:00</td>\n",
       "      <td>2017-12-11 13:30:00</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-12 01:55:00</td>\n",
       "      <td>2017-12-12 13:30:00</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-12 22:10:00</td>\n",
       "      <td>2017-12-13 13:30:00</td>\n",
       "      <td>investingnews.com</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-14 11:42:00</td>\n",
       "      <td>2017-12-14 13:30:00</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       news_timestamp     stock_timestamp             source  \\\n",
       "0 2017-12-07 20:00:00 2017-12-07 21:30:00   businesswire.com   \n",
       "1 2017-12-08 21:37:00 2017-12-11 13:30:00          yahoo.com   \n",
       "2 2017-12-12 01:55:00 2017-12-12 13:30:00          yahoo.com   \n",
       "3 2017-12-12 22:10:00 2017-12-13 13:30:00  investingnews.com   \n",
       "4 2017-12-14 11:42:00 2017-12-14 13:30:00   seekingalpha.com   \n",
       "\n",
       "                                            features label  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n",
       "2  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, ...    -1  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    -1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print sample frames\n",
    "labelled_aapl_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['access', 'accord', 'account', 'acronym', 'across', 'act', 'action', 'actual', 'ad', 'add', 'adjust', 'advisor', 'ago', 'ahead', 'ai', 'aim', 'alert', 'allow', 'almost', 'along', 'alphabet', 'also', 'although', 'america', 'american', 'amid', 'among', 'analyst', 'annual', 'appear']\n",
      "Amzn one-gram feature vector size: 19958 X 577\n"
     ]
    }
   ],
   "source": [
    "amzn_one_gram_vectorizer = CountVectorizer(max_df=0.9, min_df=0.01, stop_words=stop_words, ngram_range=(1,1))\n",
    "amzn_one_gram_features = amzn_one_gram_vectorizer.fit_transform(corpus_amzn)\n",
    "\n",
    "#printing first 30 feature words for visualization\n",
    "print(amzn_one_gram_vectorizer.get_feature_names()[:30])\n",
    "\n",
    "labelled_amzn_news_df = labelled_amzn_news_df.rename(columns={'tokens': 'features'})\n",
    "\n",
    "amzn_one_gram_features_array = amzn_one_gram_features.toarray()\n",
    "print(\"Amzn one-gram feature vector size: %d X %d\" % (len(amzn_one_gram_features_array), len(amzn_one_gram_features_array[0])))\n",
    "\n",
    "for i in range(len(amzn_one_gram_features_array)):\n",
    "    labelled_amzn_news_df.at[i,'features'] = amzn_one_gram_features_array[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_timestamp</th>\n",
       "      <th>stock_timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-15 12:04:00</td>\n",
       "      <td>2017-12-15 13:30:00</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-21 12:04:00</td>\n",
       "      <td>2017-12-21 13:30:00</td>\n",
       "      <td>seekingalpha.com</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-21 12:27:00</td>\n",
       "      <td>2017-12-21 13:30:00</td>\n",
       "      <td>thestreet.com</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-22 00:00:00</td>\n",
       "      <td>2017-12-22 13:30:00</td>\n",
       "      <td>investors.com</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-22 20:57:00</td>\n",
       "      <td>2017-12-26 13:30:00</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       news_timestamp     stock_timestamp            source  \\\n",
       "0 2017-12-15 12:04:00 2017-12-15 13:30:00  seekingalpha.com   \n",
       "1 2017-12-21 12:04:00 2017-12-21 13:30:00  seekingalpha.com   \n",
       "2 2017-12-21 12:27:00 2017-12-21 13:30:00     thestreet.com   \n",
       "3 2017-12-22 00:00:00 2017-12-22 13:30:00     investors.com   \n",
       "4 2017-12-22 20:57:00 2017-12-26 13:30:00         yahoo.com   \n",
       "\n",
       "                                            features label  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...     1  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...    -1  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    -1  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    -1  \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print sample frames\n",
    "labelled_amzn_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save one-gram features labelled data\n",
    "\n",
    "with open('aapl_one_gram_features_labelled_df_four_hour.pkl', 'wb') as f:\n",
    "    pickle.dump(labelled_aapl_news_df, f)\n",
    "with open('amzn_one_gram_features_labelled_df_four_hour.pkl', 'wb') as f:\n",
    "    pickle.dump(labelled_amzn_news_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa salt', 'aal loser', 'ab nile', 'abb cost', 'access new', 'access price', 'access rang', 'access track', 'accord add', 'accord adrian', 'accord affect', 'accord august', 'accord buy', 'accord concern', 'accord corp', 'accord display', 'accord dow', 'accord drone', 'accord gain', 'accord greater', 'accord group', 'accord maestri', 'accord maxim', 'accord ming', 'accord music', 'accord network', 'accord new', 'accord occur', 'accord phone', 'accord price']\n",
      "Aapl two-gram feature vector size: 72122 X 5394\n"
     ]
    }
   ],
   "source": [
    "#extracting two-gram features now\n",
    "\n",
    "aapl_two_gram_vectorizer = CountVectorizer(max_df=0.9, min_df=0.001, stop_words=stop_words, ngram_range=(2,2))\n",
    "aapl_two_gram_features = aapl_two_gram_vectorizer.fit_transform(corpus_aapl)\n",
    "\n",
    "#printing first 30 feature words for visualization\n",
    "print(aapl_two_gram_vectorizer.get_feature_names()[:30])\n",
    "\n",
    "aapl_two_gram_features_array = aapl_two_gram_features.toarray()\n",
    "print(\"Aapl two-gram feature vector size: %d X %d\" % (len(aapl_two_gram_features_array), len(aapl_two_gram_features_array[0])))\n",
    "\n",
    "for i in range(len(aapl_two_gram_features_array)):\n",
    "    labelled_aapl_news_df.at[i,'features'] = aapl_two_gram_features_array[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save two-gram features labelled data for Apple\n",
    "\n",
    "with open('aapl_two_gram_features_labelled_df_four_hour.pkl', 'wb') as f:\n",
    "    pickle.dump(labelled_aapl_news_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa salt', 'aal loser', 'ab nile', 'abb cost', 'access new', 'accord add', 'accord affect', 'accord amazon', 'accord buy', 'accord cloud', 'accord concern', 'accord corp', 'accord gain', 'accord group', 'accord quarter', 'accord result', 'accord support', 'accord thank', 'account alphabet', 'account invest', 'account time', 'acor lesson', 'acronym alphabet', 'acronym pick', 'acronym time', 'across prime', 'act sam', 'action market', 'action said', 'actual also']\n",
      "Amzn two-gram feature vector size: 19958 X 4047\n"
     ]
    }
   ],
   "source": [
    "amzn_two_gram_vectorizer = CountVectorizer(max_df=0.9, min_df=0.001, stop_words=stop_words, ngram_range=(2,2))\n",
    "amzn_two_gram_features = amzn_two_gram_vectorizer.fit_transform(corpus_amzn)\n",
    "\n",
    "#printing first 30 feature words for visualization\n",
    "print(amzn_two_gram_vectorizer.get_feature_names()[:30])\n",
    "\n",
    "amzn_two_gram_features_array = amzn_two_gram_features.toarray()\n",
    "print(\"Amzn two-gram feature vector size: %d X %d\" % (len(amzn_two_gram_features_array), len(amzn_two_gram_features_array[0])))\n",
    "\n",
    "for i in range(len(amzn_two_gram_features_array)):\n",
    "    labelled_amzn_news_df.at[i,'features'] = amzn_two_gram_features_array[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save two-gram features labelled data for Amazon\n",
    "\n",
    "with open('amzn_two_gram_features_labelled_df_four_hour.pkl', 'wb') as f:\n",
    "    pickle.dump(labelled_amzn_news_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
