{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"SWM_project-Copy1.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"DieP_VXhc6Ji"},"source":["import nltk\n","import gensim\n","import spacy\n","import tensorflow\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FebFlYajc6Jk"},"source":["import numpy as np\n","import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Yg4PWNkc6Jk"},"source":["with open('AaplLabelledNewsData-OrderPreserved.pkl', 'rb') as f:\n","    labelled_aapl_news_df = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qhjKdWdoc6Jk"},"source":["with open('AmznLabelledNewsData-OrderPreserved.pkl', 'rb') as f:\n","    labelled_amzn_news_df = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CCWEvWUic6Jl"},"source":["# with open('vectorized-appl.pkl', 'rb') as f:\n","#     vectorized_aapl = pickle.load(f)\n","    \n","# with open('vectorized-amzn.pkl', 'rb') as f:\n","#     vectorized_amzn = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xWcFcm80c6Jl"},"source":["#Loading word embeddings\n","from gensim import models\n","\n","w = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ya-cQpk1c6Jl"},"source":["#Tokenizing word embeddings\n","\n","from nltk.tokenize import RegexpTokenizer\n","#nltk.download('punkt')\n","tokenizer = RegexpTokenizer(r'\\w+')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cck8vuVuc6Jl"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g2V0n6WRc6Jm"},"source":["def get_sums1(row):\n","    s1 = row['tokens']\n","    #print(s1)\n","    sum1 = np.zeros((300,)) \n","    count=len(s1)\n","    \n","    for j in s1:\n","        \n","        \n","        for i in j:\n","            if i not in w: #see if a word in a title is there in the word embedding\n","                #print(i)\n","                continue\n","            sum1 = np.add(sum1,w[i])# getting the word embeddings values\n","            #increment count value if the word is present in word embedding\n","    return sum1/count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PS5aqVhdc6Jm","outputId":"f3163330-cf3c-4bba-9d2d-8a941e20f6c6"},"source":["jk = labelled_aapl_news_df.apply(get_sums1, axis=1) #apply the getsum function on train_data by each row"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n","  app.launch_new_instance()\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"y_3hlyL6c6Jn","outputId":"85693289-f997-4b05-d75c-41a45d1b77d1"},"source":["len(labelled_aapl_news_df['tokens'][0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["26"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"q2-x2SHNc6Jn","outputId":"c172cdd9-1e6e-47c3-e4c3-f56a3c943fe2"},"source":["jk2 = labelled_amzn_news_df.apply(get_sums1, axis=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n","  app.launch_new_instance()\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"pyWW04aCc6Jn"},"source":["def arraytoDf(jk,filename):\n","    row1 = pd.DataFrame(jk[0]).T\n","    count=1\n","    for mat in jk2:\n","        row2 = pd.DataFrame(mat).T\n","        row1 = row1.append(row2)\n","        count+=1\n","        if( count%1000 == 1):\n","            print(count)\n","    row1.to_pickle(filename+\".pkl\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_94bMDdFc6Jn","outputId":"96ea2992-77f5-4301-9992-55a379116d07"},"source":["arraytoDf(jk,\"glove-vectorized-aapl\")\n","arraytoDf(jk2,\"glove-vectorized-amzn\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1001\n","2001\n","3001\n","4001\n","5001\n","6001\n","7001\n","8001\n","9001\n","10001\n","11001\n","12001\n","13001\n","14001\n","15001\n","16001\n","17001\n","18001\n","19001\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M_LovoRmc6Jo"},"source":["# with open('vectorized-amzn.pkl', 'rb') as f:\n","#     vectorized_amzn = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1XjXtLhDc6Jo"},"source":["# vectorized_amzn = vectorized_amzn[1:]\n","# vectorized_amzn.to_pickle('vectorized-amzn.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MVWiFB1ac6Jo"},"source":[""],"execution_count":null,"outputs":[]}]}